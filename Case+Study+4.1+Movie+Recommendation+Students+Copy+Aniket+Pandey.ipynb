{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style =\"float:right\" src = \"https://d1vwxdpzbgdqj.cloudfront.net/s3-public-images/learning-partners/greatlearning-brand.svg\" width=15%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Case-Study-4.1:-Create-a-Movie-Recommender\">Case Study 4.1: Create a Movie Recommender<a class=\"anchor-link\" href=\"https://courses.edx.org/asset-v1:MITxPRO+DSx+2T2019+type@asset+block@case_study_4.1_notebook_python_beginner.html#Case-Study-4.1:-Create-a-Movie-Recommender\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "### One of the first steps in any data science task is importing the necessary tools you will use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: If !pip install scikit-surprise doesn't work, please install using the anconda prompt by typing the following command\n",
    "      conda install -c conda-forge scikit-surprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise import  SVD, NormalPredictor, KNNBasic\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 3 columns):\n",
      "user_id    100000 non-null int64\n",
      "item_id    100000 non-null int64\n",
      "rating     100000 non-null int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 2.3 MB\n"
     ]
    }
   ],
   "source": [
    "col_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "data = pd.read_table('u.data', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "data = data.drop('timestamp', axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Describe the dataset. How many ratings are in the dataset? How would you describe the distribution of ratings? Is there anything else we should observe? Make sure the histogram is visible in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0      196      242       3\n",
       "1      186      302       3\n",
       "2       22      377       1\n",
       "3      244       51       2\n",
       "4      166      346       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.rating.hist(bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "Dataset has about 100000 rows and 3 columns i.e 300000 entries. There are no Nulls in the dataset. The ratings are ranging from 1 to 5.\n",
    "\n",
    "Data seems to have highest number of Rating '4'. Rating '3' is second highest followed by of Rating '5'. Rating '1' is lowest with Rating '2' slightly greater but both being compratively less than the Rating '3','4' and '5'.\n",
    "\n",
    "So we can say seeing the histogram that the rating is skewed to the higher side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "\n",
    "We will be using cross validation a lot in this code in the training and evaluation of our models. This strategy builds upon the idea of a train-test split, which you should already be familiar with.\n",
    "\n",
    "Instead of doing 1 data split, though, we will do several of them. Each split of the data is called a fold. We let k denote the number of folds we use. k=5 is a common number to use.\n",
    "\n",
    "This image provides a visual explanation of how cross validation works.\n",
    "\n",
    "<img src =\"https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use \"cross_validate\" from surprise package to run the models as listed and check their respective RMSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coverting data in to surprise dataset\n",
    "reader = Reader(rating_scale=(1,5))\n",
    "data = Dataset.load_from_df(data[['user_id', 'item_id', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Random\n",
    "\n",
    "We want to first get a baseline value for our model. What better way to do that than with a random algorithm! Essentially, this first algorithm is not personalized to the desires of any users - we just assign them movie ratings based on the initial distribution of the data.\n",
    "\n",
    "See the Model 1: Random section of your notebook and follow the instructions to create a new model, train it on the data and evaluate the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm predicting a random rating based on the distribution of the training set, which is assumed to be normal.\n",
    "model_random = NormalPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_using_normal_predictor = cross_validate(model_random, data,measures=['RMSE'],cv=5, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE for Test Set using NormalPredictor is  1.5234440663365636\n"
     ]
    }
   ],
   "source": [
    "print('Average RMSE for Test Set using {} is '.format(model_random.__class__.__name__),model_using_normal_predictor['test_rmse'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: User-Based Collaborative Filtering\n",
    "\n",
    "Surely, we can do much better than guessing the movie ratings randomly! Our next model will use the user-user defined notion of similarity to implement collaborative filtering.\n",
    "\n",
    "See the Model 2: User-Based Collaborative Filtering section of your notebook and follow the instructions to create a new model, train it on the data and evaluate the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " We will use KNNBasic and add parameter 'cosine' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_user_based = KNNBasic(sim_options ={'name':'cosine','user_base':True},verbose=False)\n",
    "model_using_KNNbasic_cos_user = cross_validate(model_user_based,data,measures=['RMSE'],cv=5, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE for Test Set using KNNBasic is  1.01763070293207\n"
     ]
    }
   ],
   "source": [
    "print('Average RMSE for Test Set using {} is '.format(model_user_based.__class__.__name__),model_using_KNNbasic_cos_user['test_rmse'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Item-Based Collaborative Filtering\n",
    "\n",
    "Our next model will use the item-item defined notion of similarity to once again implement collaborative filtering.\n",
    "\n",
    "See the Model 3: Item-Based Collaborative Filtering section of your notebook and follow the instructions to create a new model, train it on the data and evaluate the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_item_based = KNNBasic(sim_options ={'name':'cosine','user_base':False},verbose=False)\n",
    "model_using_KNNbasic_cos_item = cross_validate(model_item_based,data,measures=['RMSE'],cv=5, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE for Test Set using KNNBasic is  1.016872597765717\n"
     ]
    }
   ],
   "source": [
    "print('Average RMSE for Test Set using {} is '.format(model_item_based.__class__.__name__),model_using_KNNbasic_cos_item['test_rmse'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tCompare the results from the user-user and item-item models. How do they compare to each other? How do they compare to our original \"random\" model? Can you provide any intuition as to why the results came out the way they did?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "User-based and Item-based Collaborative Models have nearly same RMSE values while the Random model's RMSE is highest.\n",
    "\n",
    "So,Collaborative Filtering Models have performed better than Random model.\n",
    "\n",
    "The Collaborative Models use the user-item-ratings data to find similarities and make predictions rather than just predicting a random rating based on the distribution of the data. This could a reason why the Collaborative Models performed well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Matrix Factorization\n",
    "\n",
    "Our final model for this case study will use the matrix factorization approach with the SVD algorithm to try to predict user’s movie ratings. Here, we try to determine some underlying mathematical structure in the user rating matrix, which can help us predict missing ratings in the future.\n",
    "\n",
    "See the Model 4: Matrix Factorization section of your notebook and follow the instructions to create a new model, train it on the data and evaluate the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix factorisation using svd\n",
    "model_svd = SVD()\n",
    "model_using_svd = cross_validate(model_svd,data,measures=['RMSE'],cv=5, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE for Test Set using SVD is  0.9362205721592216\n"
     ]
    }
   ],
   "source": [
    "print('Average RMSE for Test Set using {} is '.format(model_svd.__class__.__name__),model_using_svd['test_rmse'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tThe matrix factorization model is different from the collaborative filtering models. Briefly describe this difference. Also, compare the RMSE again. Does it improve? Can you offer any reasoning as to why that might be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "Collaborative Filtering searches for neighbors based on similarity of product preferences and recommend product that those neighbors bought/reviewed while Matrix factorization works by decomposing the user-item matrix into the product of two lower dimensionality rectangular matrices.\n",
    "\n",
    "RMSE for Matrix Factorization is slightly better than the Collaborative Filtering Models.\n",
    "\n",
    "Matrix Factorization has lower RMSE due to the reason that it assumes that both product and users are present in some low dimensional space describing their properties and recommend a product based on its proximity to the user in the latent space. Implying it accounts for latent factors as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall @ k\n",
    "\n",
    "RMSE is not the only metric we can use here. We can also examine two fundamental measures, precision and recall. We also add a parameter k which is helpful in understanding problems with multiple rating outputs.\n",
    "\n",
    "See the Precision and Recall @ k section of your notebook and follow the instructions to compute various precision/recall values at various values of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tCompute the precision and recall, for each of the 4 models, at k = 5 and 10. This is 2 x 2 x 4 = 16 numerical values. Do you note anything interesting about these values? Anything different from the RMSE values you computed above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function can be found on surprise documentation FAQs\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A basic cross-validation iterator.\n",
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> k=5, model=NormalPredictor\n",
      "-----> Precision:  0.57\n",
      "-----> Recall:  0.321\n",
      "> k=5, model=KNNBasic\n",
      "-----> Precision:  0.737\n",
      "-----> Recall:  0.437\n",
      "> k=5, model=KNNBasic\n",
      "-----> Precision:  0.74\n",
      "-----> Recall:  0.435\n",
      "> k=5, model=SVD\n",
      "-----> Precision:  0.728\n",
      "-----> Recall:  0.411\n",
      "> k=10, model=NormalPredictor\n",
      "-----> Precision:  0.57\n",
      "-----> Recall:  0.414\n",
      "> k=10, model=KNNBasic\n",
      "-----> Precision:  0.707\n",
      "-----> Recall:  0.579\n",
      "> k=10, model=KNNBasic\n",
      "-----> Precision:  0.706\n",
      "-----> Recall:  0.578\n",
      "> k=10, model=SVD\n",
      "-----> Precision:  0.704\n",
      "-----> Recall:  0.545\n"
     ]
    }
   ],
   "source": [
    "# Make list of k values\n",
    "K = [5, 10]\n",
    "\n",
    "# Make list of models\n",
    "models = [model_random, model_user_based, model_item_based, model_svd]\n",
    "\n",
    "for k in K:\n",
    "    for model in models:\n",
    "        print('> k={}, model={}'.format(k,model.__class__.__name__))\n",
    "        p = []\n",
    "        r = []\n",
    "        for trainset, testset in kf.split(data):\n",
    "            model.fit(trainset)\n",
    "            predictions = model.test(testset, verbose=False)\n",
    "            precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=3.5)\n",
    "\n",
    "            # Precision and recall can then be averaged over all users\n",
    "            p.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "            r.append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "        \n",
    "        print('-----> Precision: ', round(sum(p) / len(p), 3))\n",
    "        print('-----> Recall: ', round(sum(r) / len(r), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "Precision quantifies the number of positive class predictions that actually belong to the positive class. \n",
    "\n",
    "Recall quantifies the number of positive class predictions made out of all positive examples in the dataset. \n",
    "\n",
    "These values are lowest for Random. Collaborative Filtering performed well in both the k values with Precision value for k=5 ~73% and with k=10 ~70%.*\n",
    "\n",
    "SVD has better RMSE but Collaborative Filtering using Item-Item or User-User have better Precision & Recall.\n",
    "\n",
    "RMSE values are used for Continuous d-type while Precision-Recall are calculated for categorical d-type using Confusion matrix. Thus cannot be compared directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-n Predictions\n",
    "\n",
    "Finally, we want to actually see what ratings the model predicts for our users. We can vary the amount of top movies we see per user by varying the value of n.\n",
    "\n",
    "See the Top-n Predictions section of your notebook and follow the instructions to compute rating predictions for some users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a href=https://surprise.readthedocs.io/en/stable/FAQ.html>documentation surprise</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = data.build_full_trainset()\n",
    "testset = trainset.build_anti_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: NormalPredictor, 196: [[785   5]\n",
      " [768   5]\n",
      " [277   5]\n",
      " [ 16   5]\n",
      " [919   5]]\n",
      "model: KNNBasic, 196: [[1189    5]\n",
      " [1500    5]\n",
      " [ 814    5]\n",
      " [1536    5]\n",
      " [1293    5]]\n",
      "model: KNNBasic, 196: [[1189    5]\n",
      " [1500    5]\n",
      " [ 814    5]\n",
      " [1536    5]\n",
      " [1293    5]]\n",
      "model: SVD, 196: [[357.     4.76]\n",
      " [178.     4.54]\n",
      " [ 50.     4.53]\n",
      " [427.     4.51]\n",
      " [127.     4.51]]\n"
     ]
    }
   ],
   "source": [
    "models = [model_random, model_user_based, model_item_based, model_svd]\n",
    "for model in models:\n",
    "    model.fit(trainset)\n",
    "    predictions = model.test(testset)\n",
    "    top_n = get_top_n(predictions, n=5)\n",
    "    # Print the first one\n",
    "    user = list(top_n.keys())[0]\n",
    "    print(f'model: {model.__class__.__name__}, {user}: {np.round(top_n[user],2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tDo the top n predictions that you received make sense? What is the rating value (1-5) of these predictions? How could you use these predictions in the real-world if you were trying to build a generic content recommender system for a company?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "As expected, we have got the predictions (n=5) for each model. We printed the predictions for One user for each model. Collaborative models  (User-based & Item-based) have given exactly same recommendations.\n",
    "\n",
    "The rating values for Random, User-based, Item-based and Matrix Factorization using SVD in fixed at 5.\n",
    "\n",
    "\n",
    "Recommender systems are used by E-commerce portals to recommend products to their customers. The products can be recommended based on the top overall sellers on a site, based on the demographics of the customer, or based on an analysis of the past buying behavior of the customer as a prediction for future buying behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3 style =\"text-align:center\"><strong> Happy Learning</strong></H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
